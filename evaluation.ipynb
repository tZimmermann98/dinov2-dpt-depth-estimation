{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "719d7080-2065-46a0-ba07-4c3f8c5db9f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "from transformers import AutoImageProcessor, DPTForDepthEstimation\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b6875231-9af4-42b1-a175-b6b47c2e8c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(98)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7b824573-daf3-4bca-8e68-aa2d884f5f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE = (256, 255)\n",
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "961a962c-e5d3-4f3b-bf2a-cdc7a7695ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskedMAE(nn.Module):\n",
    "    def __init__(self, valid_mask=True, max_depth=None):\n",
    "        super(MaskedMAE, self).__init__()\n",
    "        \n",
    "        self.valid_mask = valid_mask\n",
    "        self.max_depth = max_depth\n",
    "\n",
    "    def mae(self, input, target):\n",
    "        if self.valid_mask:\n",
    "            valid_mask = target > 0\n",
    "            if self.max_depth is not None:\n",
    "                valid_mask = torch.logical_and(target > 0, target <= self.max_depth)\n",
    "            input = input[valid_mask]\n",
    "            target = target[valid_mask]\n",
    "\n",
    "        mae = torch.abs(input - target).mean()\n",
    "        return mae\n",
    "    \n",
    "    def forward(self, depth_pred, depth_gt):\n",
    "        metric_mae = self.mae(depth_pred, depth_gt)\n",
    "        return metric_mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7273ecc9-4e9e-4d50-8b0a-1225d420e083",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskedR2Score(nn.Module):\n",
    "    def __init__(self, valid_mask=True, max_depth=None):\n",
    "        super(MaskedR2Score, self).__init__()\n",
    "\n",
    "        self.valid_mask = valid_mask\n",
    "        self.max_depth = max_depth\n",
    "\n",
    "    def r2(self, input, target):\n",
    "        if self.valid_mask:\n",
    "            valid_mask = target > 0\n",
    "            if self.max_depth is not None:\n",
    "                valid_mask = torch.logical_and(target > 0, target <= self.max_depth)\n",
    "            input = input[valid_mask]\n",
    "            target = target[valid_mask]\n",
    "\n",
    "        mean_target = torch.mean(target)\n",
    "        ss_total = torch.sum((target - mean_target)**2)\n",
    "        ss_residual = torch.sum((input - target)**2)\n",
    "\n",
    "        r2 = 1 - (ss_residual / ss_total)\n",
    "        return r2\n",
    "    \n",
    "    def forward(self, depth_pred, depth_gt):\n",
    "        metric_r2 = self.r2(depth_pred, depth_gt)\n",
    "        return metric_r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5245d004-9867-437f-b454-8e9c7f0f84df",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomNPZDataset(Dataset):\n",
    "    def __init__(self, path, transform=None, image_transforms=None):\n",
    "        self.path = path\n",
    "        self.files = list(Path(path).glob('*.npz'))\n",
    "        self.transform = transform\n",
    "        self.image_transforms = image_transforms\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        with np.load(str(self.files[item])) as data:\n",
    "            X_numpy = data['X']\n",
    "            y_numpy = data['y']\n",
    "        X_torch = torch.from_numpy(X_numpy)\n",
    "        y_torch = torch.from_numpy(y_numpy).unsqueeze(0)\n",
    "        if self.transform is not None:\n",
    "            X_torch = self.transform(X_torch)\n",
    "            y_torch = self.transform(y_torch)\n",
    "        if self.image_transforms is not None:\n",
    "            X_torch = self.transform(X_torch)\n",
    "        return X_torch, y_torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0820319c-a4bc-4d88-9c39-906d5928dd72",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    \"KITTI\": \"facebook/dpt-dinov2-small-kitti\",\n",
    "    \"NYUd\": \"facebook/dpt-dinov2-small-nyu\",\n",
    "    \"Scratch\": \"model\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a11d19f8-7e79-474e-88d3-e3730fc91488",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG_NAME = \"facebook/dinov2-small\"\n",
    "image_processor = AutoImageProcessor.from_pretrained(CONFIG_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03b8f721-ec12-4207-86ae-92ed99450cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_PATH = \"data/thumbnails/test\"\n",
    "validation_dataset = CustomNPZDataset(path=TEST_PATH)\n",
    "validation_loader = DataLoader(validation_dataset, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3b06e19-5cd9-4555-9ffd-afa29fc04e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "mae_fn = MaskedMAE()\n",
    "r2_fn = MaskedR2Score()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc412641-fc5d-40b3-b8a8-991ce72e6cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model(data):\n",
    "    inputs, labels = data\n",
    "\n",
    "    images = [Image.fromarray(input.numpy().transpose(1, 2, 0)) for input in inputs]\n",
    "\n",
    "    inputs = image_processor(images=images, return_tensors=\"pt\")\n",
    "\n",
    "    inputs = inputs.to(DEVICE)\n",
    "    \n",
    "    outputs = model(**inputs)\n",
    "\n",
    "    predicted_depth = outputs['predicted_depth']\n",
    "\n",
    "    predictions = torch.nn.functional.interpolate(\n",
    "        predicted_depth.unsqueeze(1),\n",
    "        size=IMAGE_SIZE,\n",
    "        mode=\"bicubic\",\n",
    "        align_corners=False,\n",
    "    )\n",
    "\n",
    "    labels = labels.to(DEVICE)\n",
    "    \n",
    "    mae = mae_fn(predictions, labels)\n",
    "    r2 = r2_fn(predictions, labels)\n",
    "    \n",
    "    return mae, r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eefb2d03-80d0-4edd-95f3-39fa98238181",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_data = {\"names\": [], \"maes\": [], \"r2s\": []}\n",
    "for name in models:\n",
    "    model = DPTForDepthEstimation.from_pretrained(models[name])\n",
    "    model.cuda()\n",
    "    model.eval()\n",
    "    running_vmae = 0.0\n",
    "    running_vr2 = 0.0\n",
    "    with torch.no_grad():\n",
    "        for i, vdata in tqdm(enumerate(validation_loader), total=len(validation_loader)):\n",
    "            vmae, vr2 = run_model(vdata)\n",
    "            running_vmae += vmae\n",
    "            running_vr2 += vr2\n",
    "    avg_vmae = running_vmae / len(validation_loader)\n",
    "    avg_vr2 = running_vr2 / len(validation_loader)\n",
    "    model_data[\"names\"].append(name)\n",
    "    model_data[\"maes\"].append(avg_vmae)\n",
    "    model_data[\"r2s\"].append(avg_vr2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d8ea57c-5e08-4c59-9201-91c313f4e2e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "maes_clean = map(lambda x: round(x.item(), 2), model_data[\"maes\"])\n",
    "r2s_clean = map(lambda x: round(x.item(), 2), model_data[\"r2s\"])\n",
    "clean_data = {\"Model\": model_data[\"names\"], \"MAE\": list(maes_clean), \"R2\": list(r2s_clean)}\n",
    "df = pd.DataFrame.from_dict(clean_data)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "121f388a-b7a7-47f5-a739-2f1680ee5158",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_images = 4\n",
    "for image in range(num_images):\n",
    "    X, y = validation_dataset[random.randint(0, len(validation_dataset)-1)]\n",
    "    images = [Image.fromarray(X.numpy().transpose(1, 2, 0))]\n",
    "    image_processor = AutoImageProcessor.from_pretrained('facebook/dinov2-small')\n",
    "    inputs = image_processor(images=images, return_tensors=\"pt\")\n",
    "    inputs = inputs.to(DEVICE)\n",
    "    fig, axes = plt.subplots(len(models), 3, figsize=(12, len(models) * 4))\n",
    "    for i, name in enumerate(models):\n",
    "        model = DPTForDepthEstimation.from_pretrained(models[name])\n",
    "        model.cuda()\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        predicted_depth = outputs['predicted_depth']\n",
    "        predictions = torch.nn.functional.interpolate(\n",
    "            predicted_depth.unsqueeze(1),\n",
    "            size=IMAGE_SIZE,\n",
    "            mode=\"bicubic\",\n",
    "            align_corners=False,\n",
    "        )\n",
    "        axes[i][0].imshow(X.numpy().transpose(1, 2, 0))\n",
    "        axes[i][0].set_title(\"Satelite Image\")\n",
    "        axes[i][1].imshow(predictions.squeeze(0).squeeze(0).cpu().numpy(), cmap='viridis')\n",
    "        axes[i][1].set_title(f\"{name} Prediction\")\n",
    "        axes[i][2].imshow(y.squeeze(0).numpy(), cmap='viridis')\n",
    "        axes[i][2].set_title(\"LiDAR Ground Truth\")\n",
    "        \n",
    "    fig.savefig(f\"image-{image}.png\", dpi=fig.dpi, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ece571a-0118-4d07-978f-eecdd1f54752",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(12, 4))\n",
    "axes[0].imshow(X.numpy().transpose(1, 2, 0))\n",
    "axes[0].set_title(\"Satelite Image\")\n",
    "axes[1].imshow(predictions.squeeze(0).squeeze(0).cpu().numpy(), cmap='viridis')\n",
    "axes[1].set_title(f\"{name} Prediction\")\n",
    "axes[2].imshow(y.squeeze(0).numpy(), cmap='viridis')\n",
    "axes[2].set_title(\"LiDAR Ground Truth\")\n",
    "\n",
    "fig.savefig(f\"image-{image}_Scratch.png\", dpi=fig.dpi, bbox_inches='tight')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
