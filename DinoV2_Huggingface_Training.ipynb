{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "391c7706-ab76-4596-892f-fb4e7ed3dc70",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "27661c2e-8034-46c3-babb-abdd92d17439",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`AnnotionFormat` is deprecated and will be removed in v4.38. Please use `transformers.image_utils.AnnotationFormat` instead.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import multiprocessing\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR, OneCycleLR\n",
    "from PIL import Image\n",
    "from transformers import AutoImageProcessor, DPTForDepthEstimation, get_scheduler\n",
    "from datetime import datetime\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b092f532-ef31-41ae-bc29-f07e82ea7e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1204e5c-2e80-4741-9d78-669e0d5d176b",
   "metadata": {},
   "source": [
    "# Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "762c3835-0237-4f83-8637-f4da217ce2e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE = (256, 255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9b9ba7b4-7dd2-4f7d-9bd6-97895377b1c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7d03ca20-f559-4006-afba-49ebff9b5b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "BACKBONE_TYPE = 'small' # in (\"small\", \"base\", \"large\" or \"giant\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e4147410-5519-4cb0-90f7-79812955e3a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "HEAD_TYPE = 'nyu' # in (\"nyu\", \"kitti\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ddca584c-ebac-468d-8099-4de9b1f9d42e",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = f\"facebook/dpt-dinov2-{BACKBONE_TYPE}-{HEAD_TYPE}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "074ae3eb-683b-4999-97f9-e8fc34c0fd79",
   "metadata": {},
   "outputs": [],
   "source": [
    "RESUME_PATH = \"HF-model_small_nyu_20231230_082544_40\"\n",
    "if RESUME_PATH is not None:\n",
    "    BACKBONE_TYPE = RESUME_PATH.split('/')[-1].split('_')[1]\n",
    "    HEAD_TYPE = RESUME_PATH.split('/')[-1].split('_')[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4af32055-55c0-445c-9b47-747a569ab49d",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_PATH = \"/home/jovyan/work/saved_data/data/thumbnails/train\"\n",
    "TEST_PATH = \"/home/jovyan/work/saved_data/data/thumbnails/test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1706c93d-69c4-4c03-b726-640c8275e676",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "EPOCHS = 100\n",
    "BATCH_SIZE = 32\n",
    "EVAL_INTERVAL = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b482e2cb-71f3-412e-828b-d100f994d1fa",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "START_LR = 1e-8\n",
    "MIN_LR = 1e-9\n",
    "MAX_LR = 1e-7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f0dff5a5-ce8e-4499-ad70-24f4ab335b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_BACKBONE = True\n",
    "TRAIN_NECK = True\n",
    "TRAIN_HEAD = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e63c15f-b8a4-457f-bc4e-80b02ad6e56d",
   "metadata": {},
   "source": [
    "# Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cf9dc4a0-e4c6-4e65-9f21-d6e169af4e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomNPZDataset(Dataset):\n",
    "    def __init__(self, path, image_processor, transform=None):\n",
    "        self.path = path\n",
    "        self.files = list(Path(path).glob('*.npz'))\n",
    "        self.transform = transform\n",
    "        self.image_processor = image_processor\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        with np.load(str(self.files[item])) as data:\n",
    "            X_numpy = data['X']\n",
    "            y_numpy = data['y']\n",
    "        X_torch = torch.from_numpy(X_numpy)\n",
    "        y_torch = torch.from_numpy(y_numpy).unsqueeze(0)\n",
    "        if self.transform is not None:\n",
    "            X_torch = self.transform(X_torch)\n",
    "            y_torch = self.transform(y_torch)\n",
    "        return X_torch, y_torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d7563cef-46c5-4417-b3c6-d60edc0f86c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SigLoss(nn.Module):\n",
    "    def __init__(\n",
    "        self, valid_mask=True, max_depth=None):\n",
    "        super(SigLoss, self).__init__()\n",
    "        \n",
    "        self.valid_mask = valid_mask\n",
    "        self.max_depth = max_depth\n",
    "\n",
    "        self.eps = 0.001  # avoid grad explode\n",
    "\n",
    "    def sigloss(self, input, target):\n",
    "        if self.valid_mask:\n",
    "            valid_mask = target > 0\n",
    "            if self.max_depth is not None:\n",
    "                valid_mask = torch.logical_and(target > 0, target <= self.max_depth)\n",
    "            input = input[valid_mask]\n",
    "            target = target[valid_mask]\n",
    "\n",
    "        g = torch.log(input + self.eps) - torch.log(target + self.eps)\n",
    "        Dg = torch.var(g) + 0.15 * torch.pow(torch.mean(g), 2)\n",
    "        return torch.sqrt(Dg)\n",
    "\n",
    "    def forward(self, depth_pred, depth_gt):\n",
    "        loss_depth = self.sigloss(depth_pred, depth_gt)\n",
    "        return loss_depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "34f1ab4f-0fac-47e0-b3a0-b73c6b75cf0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskedMAE(nn.Module):\n",
    "    def __init__(self, valid_mask=True, max_depth=None):\n",
    "        super(MaskedMAE, self).__init__()\n",
    "        \n",
    "        self.valid_mask = valid_mask\n",
    "        self.max_depth = max_depth\n",
    "\n",
    "    def mae(self, input, target):\n",
    "        if self.valid_mask:\n",
    "            valid_mask = target > 0\n",
    "            if self.max_depth is not None:\n",
    "                valid_mask = torch.logical_and(target > 0, target <= self.max_depth)\n",
    "            input = input[valid_mask]\n",
    "            target = target[valid_mask]\n",
    "\n",
    "        mae = torch.abs(input - target).mean()\n",
    "        return mae\n",
    "    \n",
    "    def forward(self, depth_pred, depth_gt):\n",
    "        metric_mae = self.mae(depth_pred, depth_gt)\n",
    "        return metric_mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "198f7754-341c-4828-a749-53381277b1bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskedR2Score(nn.Module):\n",
    "    def __init__(self, valid_mask=True, max_depth=None):\n",
    "        super(MaskedR2Score, self).__init__()\n",
    "\n",
    "        self.valid_mask = valid_mask\n",
    "        self.max_depth = max_depth\n",
    "\n",
    "    def r2(self, input, target):\n",
    "        if self.valid_mask:\n",
    "            valid_mask = target > 0\n",
    "            if self.max_depth is not None:\n",
    "                valid_mask = torch.logical_and(target > 0, target <= self.max_depth)\n",
    "            input = input[valid_mask]\n",
    "            target = target[valid_mask]\n",
    "\n",
    "        mean_target = torch.mean(target)\n",
    "        ss_total = torch.sum((target - mean_target)**2)\n",
    "        ss_residual = torch.sum((input - target)**2)\n",
    "\n",
    "        r2 = 1 - (ss_residual / ss_total)\n",
    "        return r2\n",
    "    \n",
    "    def forward(self, depth_pred, depth_gt):\n",
    "        metric_r2 = self.r2(depth_pred, depth_gt)\n",
    "        return metric_r2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60180a32-d5b2-4de8-a20d-49d0e64b8ffd",
   "metadata": {},
   "source": [
    "# Initiallization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cffa742b-1f9d-4fc3-9a81-1e25265ffb74",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_processor = AutoImageProcessor.from_pretrained(MODEL_NAME)\n",
    "if RESUME_PATH is None:\n",
    "    model = DPTForDepthEstimation.from_pretrained(MODEL_NAME)\n",
    "else:\n",
    "    model = DPTForDepthEstimation.from_pretrained(RESUME_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "87b7f811-d922-4c50-8236-8573bc390f05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DPTForDepthEstimation(\n",
       "  (backbone): Dinov2Backbone(\n",
       "    (embeddings): Dinov2Embeddings(\n",
       "      (patch_embeddings): Dinov2PatchEmbeddings(\n",
       "        (projection): Conv2d(3, 384, kernel_size=(14, 14), stride=(14, 14))\n",
       "      )\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (encoder): Dinov2Encoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x Dinov2Layer(\n",
       "          (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          (attention): Dinov2Attention(\n",
       "            (attention): Dinov2SelfAttention(\n",
       "              (query): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (key): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (value): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (output): Dinov2SelfOutput(\n",
       "              (dense): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (layer_scale1): Dinov2LayerScale()\n",
       "          (drop_path1): Identity()\n",
       "          (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Dinov2MLP(\n",
       "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          )\n",
       "          (layer_scale2): Dinov2LayerScale()\n",
       "          (drop_path2): Identity()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (layernorm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "  )\n",
       "  (neck): DPTNeck(\n",
       "    (reassemble_stage): DPTReassembleStage(\n",
       "      (layers): ModuleList(\n",
       "        (0): DPTReassembleLayer(\n",
       "          (projection): Conv2d(384, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (resize): ConvTranspose2d(48, 48, kernel_size=(4, 4), stride=(4, 4))\n",
       "        )\n",
       "        (1): DPTReassembleLayer(\n",
       "          (projection): Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (resize): ConvTranspose2d(96, 96, kernel_size=(2, 2), stride=(2, 2))\n",
       "        )\n",
       "        (2): DPTReassembleLayer(\n",
       "          (projection): Conv2d(384, 192, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (resize): Identity()\n",
       "        )\n",
       "        (3): DPTReassembleLayer(\n",
       "          (projection): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (resize): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (readout_projects): ModuleList(\n",
       "        (0-3): 4 x Sequential(\n",
       "          (0): Linear(in_features=768, out_features=384, bias=True)\n",
       "          (1): GELUActivation()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (convs): ModuleList(\n",
       "      (0): Conv2d(48, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (1): Conv2d(96, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (2): Conv2d(192, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (3): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    )\n",
       "    (fusion_stage): DPTFeatureFusionStage(\n",
       "      (layers): ModuleList(\n",
       "        (0-3): 4 x DPTFeatureFusionLayer(\n",
       "          (projection): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (residual_layer1): DPTPreActResidualLayer(\n",
       "            (activation1): ReLU()\n",
       "            (convolution1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (activation2): ReLU()\n",
       "            (convolution2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          )\n",
       "          (residual_layer2): DPTPreActResidualLayer(\n",
       "            (activation1): ReLU()\n",
       "            (convolution1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (activation2): ReLU()\n",
       "            (convolution2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (head): DPTDepthEstimationHead(\n",
       "    (projection): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (head): Sequential(\n",
       "      (0): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): Upsample(scale_factor=2.0, mode='bilinear')\n",
       "      (2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (3): ReLU()\n",
       "      (4): Conv2d(32, 1, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (5): ReLU()\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5faca390-48c6-4934-a64f-57a48c99b8b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "augmentation_transform = transforms.Compose([\n",
    "    transforms.RandomVerticalFlip(),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(90)\n",
    "    #transforms.RandomResizedCrop((IMAGE_SIZE[0], IMAGE_SIZE[1]), scale=(0.8, 1.0)),\n",
    "    #transforms.ColorJitter(brightness=0.4, contrast=0.2)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "aaf7c9af-2223-436b-b04a-ee21017764c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CustomNPZDataset(path=TRAIN_PATH, image_processor=image_processor, transform=augmentation_transform)\n",
    "validation_dataset = CustomNPZDataset(path=TEST_PATH, image_processor=image_processor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d800fc17-c250-420f-9b89-27f93e8ed83a",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_loader = DataLoader(train_dataset, shuffle=True, batch_size=BATCH_SIZE, num_workers=10, pin_memory=True)\n",
    "validation_loader = DataLoader(validation_dataset, batch_size=BATCH_SIZE, num_workers=8, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8b0c9d4b-ad27-42d4-882e-c27a0f70af4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = AdamW(model.parameters(), lr=START_LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c6639c55-cc99-41eb-8457-21da3f64d188",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_steps = EPOCHS * len(training_loader)\n",
    "#lr_scheduler = CosineAnnealingLR(optimizer, total_steps, eta_min=MIN_LR)\n",
    "lr_scheduler = OneCycleLR(optimizer, MAX_LR, total_steps=total_steps, anneal_strategy='linear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3284ecde-e82e-4f9a-8d4f-6f4515377e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = SigLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2ac56f2d-0b69-492b-a9a6-737125d25d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "mae_fn = MaskedMAE()\n",
    "r2_fn = MaskedR2Score()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f615045-8e7a-4821-a526-8197a9ce0bc9",
   "metadata": {},
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "82de64be-b0c8-47e9-b4ad-5d78b538d494",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model(data):\n",
    "    inputs, labels = data\n",
    "\n",
    "    images = [Image.fromarray(input.numpy().transpose(1, 2, 0)) for input in inputs]\n",
    "\n",
    "    inputs = image_processor(images=images, return_tensors=\"pt\")\n",
    "\n",
    "    inputs = inputs.to(DEVICE)\n",
    "    \n",
    "    outputs = model(**inputs)\n",
    "\n",
    "    predicted_depth = outputs['predicted_depth']\n",
    "\n",
    "    predictions = torch.nn.functional.interpolate(\n",
    "        predicted_depth.unsqueeze(1),\n",
    "        size=IMAGE_SIZE,\n",
    "        mode=\"bicubic\",\n",
    "        align_corners=False,\n",
    "    )\n",
    "\n",
    "    del inputs, outputs, predicted_depth\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    labels = labels.to(DEVICE)\n",
    "    \n",
    "    loss = loss_fn(predictions, labels)\n",
    "\n",
    "    mae = mae_fn(predictions, labels)\n",
    "    r2 = r2_fn(predictions, labels)\n",
    "\n",
    "    del predictions, labels\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    return loss, mae, r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "27b95a49-e31f-44d5-8b17-2045f115d485",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_one_epoch():\n",
    "    running_loss = 0.\n",
    "    epoch_loss = 0.\n",
    "\n",
    "    running_mae = 0.\n",
    "    running_r2 = 0.\n",
    "    epoch_mae = 0.\n",
    "    epoch_r2 = 0.\n",
    "\n",
    "    for i, data in tqdm(enumerate(training_loader), total=len(training_loader)):\n",
    "        loss, mae, r2 = run_model(data)\n",
    "\n",
    "        if loss.isnan().all():\n",
    "            raise Exception('Exploding Gradients!')\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 5.0)\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        running_mae += mae.item()\n",
    "        running_r2 += r2.item()\n",
    "\n",
    "    epoch_loss = running_loss / len(training_loader)\n",
    "\n",
    "    epoch_mae = running_mae / len(training_loader)\n",
    "    epoch_r2 = running_r2 / len(training_loader)\n",
    "    \n",
    "    return epoch_loss, epoch_mae, epoch_r2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4b97d5f-9309-4048-a5fc-8f296fed2bd8",
   "metadata": {},
   "source": [
    "# Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "592acf19-704b-4185-a4ea-f6069614fbb4",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 1:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f4f2f824e6c4695a4b9344644576a7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/120 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOSS Train 1.9611224134763081\n",
      "MAE Train 8.310295307636261\n",
      "R2 Train -0.5459240287542343\n",
      "EPOCH 2:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70eb5330f7a448fbbfe00053ce483e8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/120 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOSS Train 1.8725258549054464\n",
      "MAE Train 7.941031324863434\n",
      "R2 Train -0.42842679619789126\n",
      "EPOCH 3:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "991770e947084cc1b5f8a070dbbe51ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/120 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOSS Train 1.8161075860261917\n",
      "MAE Train 7.657207369804382\n",
      "R2 Train -0.32806953142086664\n",
      "EPOCH 4:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1d1e88f67a54e9b8e95136534b95467",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/120 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOSS Train 1.7852516978979112\n",
      "MAE Train 7.428497409820556\n",
      "R2 Train -0.24537241458892822\n",
      "EPOCH 5:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62793976f8bd43f3b9334cc99fe0d9ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/120 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOSS Train 1.763675394654274\n",
      "MAE Train 7.198892585436503\n",
      "R2 Train -0.17159229069948195\n",
      "EPOCH 6:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d7458c5bec54f3d9939642cffb5726c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/120 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOSS Train 1.7451706101497015\n",
      "MAE Train 7.043346099058787\n",
      "R2 Train -0.12012081742286682\n",
      "EPOCH 7:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e0ede39895b4ec491bfa61e31f32c80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/120 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOSS Train 1.7353169898192087\n",
      "MAE Train 6.910624627272288\n",
      "R2 Train -0.08063958982626597\n",
      "EPOCH 8:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec5be143420b4c9f9d8f7b46f676b6f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/120 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOSS Train 1.7209773361682892\n",
      "MAE Train 6.80936926205953\n",
      "R2 Train -0.05071255515019099\n",
      "EPOCH 9:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b513aad52519468798403b6cb1405efd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/120 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOSS Train 1.7176567941904068\n",
      "MAE Train 6.693831300735473\n",
      "R2 Train -0.013770333925882975\n",
      "EPOCH 10:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d88308c3c85e4bca9b33b79c6119be9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/120 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOSS Train 1.7107423812150955\n",
      "MAE Train 6.626129305362701\n",
      "R2 Train 0.00046727657318115237\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "942d0fe56ad34a6c8c5f483c5413c7d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/140 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOSS valid nan\n",
      "MAE valid 7.8555707931518555\n",
      "R2 valid -0.07703547924757004\n",
      "EPOCH 11:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84e3781d75254fd7b4147cc925a4217f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/120 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "Exception",
     "evalue": "Exploding Gradients!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 20\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m param \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mhead\u001b[38;5;241m.\u001b[39mparameters():\n\u001b[1;32m     18\u001b[0m     param\u001b[38;5;241m.\u001b[39mrequires_grad \u001b[38;5;241m=\u001b[39m TRAIN_HEAD\n\u001b[0;32m---> 20\u001b[0m avg_loss, avg_mae, avg_r2 \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLOSS Train \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(avg_loss))\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMAE Train \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(avg_mae))\n",
      "Cell \u001b[0;32mIn[27], line 14\u001b[0m, in \u001b[0;36mtrain_one_epoch\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m loss, mae, r2 \u001b[38;5;241m=\u001b[39m run_model(data)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m loss\u001b[38;5;241m.\u001b[39misnan()\u001b[38;5;241m.\u001b[39mall():\n\u001b[0;32m---> 14\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mExploding Gradients!\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     16\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     18\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "\u001b[0;31mException\u001b[0m: Exploding Gradients!"
     ]
    }
   ],
   "source": [
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "writer = SummaryWriter(f\"runs/dinov2_{BACKBONE_TYPE}_dpt_{HEAD_TYPE}_{timestamp}\")\n",
    "\n",
    "epoch_number = 1\n",
    "\n",
    "best_vloss = 1_000_000.\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print('EPOCH {}:'.format(epoch_number))\n",
    "\n",
    "    model.train(True)\n",
    "    \n",
    "    for param in model.backbone.parameters():\n",
    "        param.requires_grad = TRAIN_BACKBONE\n",
    "    for param in model.neck.parameters():\n",
    "        param.requires_grad = TRAIN_NECK\n",
    "    for param in model.head.parameters():\n",
    "        param.requires_grad = TRAIN_HEAD\n",
    "        \n",
    "    avg_loss, avg_mae, avg_r2 = train_one_epoch()\n",
    "\n",
    "    print('LOSS Train {}'.format(avg_loss))\n",
    "\n",
    "    print('MAE Train {}'.format(avg_mae))\n",
    "    print('R2 Train {}'.format(avg_r2))\n",
    "\n",
    "    writer.add_scalars('Training Loss',\n",
    "                        { 'Training' : avg_loss },\n",
    "                        epoch_number)\n",
    "    writer.add_scalars('Training MAE',\n",
    "                    { 'Training' : avg_mae },\n",
    "                    epoch_number)\n",
    "    writer.add_scalars('Training R2',\n",
    "                    { 'Training' : avg_r2 },\n",
    "                    epoch_number)\n",
    "\n",
    "    if epoch_number % EVAL_INTERVAL == 0:\n",
    "        running_vloss = 0.0\n",
    "\n",
    "        running_vmae = 0.0\n",
    "        running_vr2 = 0.0\n",
    "    \n",
    "        model.eval()\n",
    "    \n",
    "        with torch.no_grad():\n",
    "            for i, vdata in tqdm(enumerate(validation_loader), total=len(validation_loader)):\n",
    "                vloss, vmae, vr2 = run_model(vdata)\n",
    "                running_vloss += vloss\n",
    "                running_vmae += vmae\n",
    "                running_vr2 += vr2\n",
    "    \n",
    "        avg_vloss = running_vloss / len(validation_loader)\n",
    "        print('LOSS valid {}'.format(avg_vloss))\n",
    "    \n",
    "        avg_vmae = running_vmae / len(validation_loader)\n",
    "        print('MAE valid {}'.format(avg_vmae))\n",
    "        avg_vr2 = running_vr2 / len(validation_loader)\n",
    "        print('R2 valid {}'.format(avg_vr2))\n",
    "    \n",
    "        writer.add_scalars('Validation Loss',\n",
    "                        { 'Validation' : avg_vloss },\n",
    "                        epoch_number)\n",
    "        writer.add_scalars('Validation MAE',\n",
    "                        { 'Validation' : avg_vmae },\n",
    "                        epoch_number)\n",
    "        writer.add_scalars('Validation R2',\n",
    "                        { 'Validation' : avg_vr2 },\n",
    "                        epoch_number)\n",
    "\n",
    "        if avg_vloss < best_vloss:\n",
    "            best_vloss = avg_vloss\n",
    "            model_path = 'model_{}_{}_{}_{}'.format(BACKBONE_TYPE, HEAD_TYPE, timestamp, epoch_number)\n",
    "            torch.save(model.state_dict(), model_path)\n",
    "            os.mkdir(f\"HF-{model_path}\")\n",
    "            model.save_pretrained(f\"HF-{model_path}\")\n",
    "    \n",
    "    writer.flush()\n",
    "    \n",
    "    epoch_number += 1\n",
    "\n",
    "    lr_scheduler.step()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
